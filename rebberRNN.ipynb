{"nbformat_minor": 0, "cells": [{"execution_count": 177, "cell_type": "code", "source": "%matplotlib inline\n%load_ext autoreload\n%autoreload 2\n\nimport numpy as np\nimport theano\nimport theano.tensor as T\nfrom theano import shared \nfrom theano.ifelse import ifelse\nfrom collections import OrderedDict\nimport matplotlib.pyplot as plt\nimport cPickle\nimport sys\nimport re\nfrom reber import ReberGrammar, EmbeddedReberGrammar\nfrom NNutils import share_or_init_weight\n\ndtype=T.config.floatX\ntheano.config.optimizer='fast_compile'", "outputs": [{"output_type": "stream", "name": "stdout", "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 90, "cell_type": "code", "source": "grammar = ReberGrammar()", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 137, "cell_type": "code", "source": "class Rnn:\n    def __init__(self, optim=None, path=None, n_in=None, n_hid=None, n_out=None, \n                 W_in=None, W_out=None, W_rec=None, b_out=None, b_h=None, h0=None):   \n        if path is not None:\n            loaded = []\n            f = file(path,'rb')\n            stop_pickling = False\n            while not stop_pickling:\n                try:\n                    loaded.append(cPickle.load(f))\n                except:\n                    stop_pickling = True\n            f.close()\n            W_in, W_out, W_rec, b_out, b_h, h0, optim = loaded\n        \n        self.optim = optim\n        self.W_in = share_or_init_weight(weights=W_in, shape=(n_in, n_hid),name='W_in', sample='svd')\n        self.W_out = share_or_init_weight(weights=W_out, shape=(n_hid, n_out),name='W_out', sample='svd')\n        self.W_rec = share_or_init_weight(weights=W_rec, shape=(n_hid, n_hid), name='W_rec', sample='svd')\n        self.b_out = share_or_init_weight(weights=b_out, shape=(n_out), name='b_out',sample='zero')\n        self.b_h = share_or_init_weight(weights=b_h, shape=(n_hid), name='b_h',sample='zero')\n        \n        self.h0 = share_or_init_weight(weights=h0, shape=(n_hid), name=\"h0\", sample='zero') # initial hidden state \n        \n        self.params = [self.W_in,self.W_out,self.W_rec, self.b_out, self.b_h]\n        \n        def step(x_t, h_tm1):\n            h_t = T.tanh(T.dot(x_t, self.W_in) + T.dot(h_tm1, self.W_rec) + self.b_h)\n            y_t = T.nnet.sigmoid((T.dot(h_t, self.W_out) + self.b_out))            \n            return [h_t, y_t]\n\n        X = T.matrix('X') # X is the sequence of vector\n        Y = T.matrix('Y') # Y is the output of vector\n        \n        [h_vals, y_vals], _ = theano.scan(fn=step,                                  \n                                          sequences=X,\n                                          outputs_info=[self.h0, None])\n        \n        cost = -T.mean(Y * T.log(y_vals)+ (1.- Y) * T.log(1. - y_vals))\n        gparams = T.grad(cost, self.params)\n        updates = OrderedDict()\n        \n        if self.optim['method'] == 'sgd':\n            lr = shared(np.cast[dtype](self.optim['learning_rate']), name='lr')\n            for param, gparam in zip(self.params, gparams):\n                updates[param] = param - gparam * lr\n            self.train = theano.function(inputs = [X, Y], outputs = cost, updates=updates)\n                \n        elif self.optim['method'] == 'momentum':\n            lr = shared(np.cast[dtype](self.optim['learning_rate']), name='lr')\n            mom_start = shared(np.cast[dtype](self.optim['mom_start']), name='mom_start')\n            mom_end = shared(np.cast[dtype](self.optim['mom_end']), name='mom_end')\n            mom_epoch_interval = shared(np.cast[dtype](self.optim['mom_epoch_interval']), name='mom_epoch_interval')\n            epoch = T.scalar('epoch')\n            \n            gparam_momentum = [share_or_init_weight(shape=param.get_value(borrow=True).shape, \n                                                    name=\"gparam_mom\", sample=\"zero\") for param in self.params]\n            mom = ifelse(epoch < mom_epoch_interval,\n                         (mom_start*(1-epoch/mom_epoch_interval) + mom_end*(epoch/mom_epoch_interval)).astype(dtype),\n                         mom_end)\n            for gparam_mom, gparam in zip(gparam_momentum, gparams):\n                updates[gparam_mom] = mom*gparam_mom + (1-mom)*gparam\n\n            for param, gparam_mom in zip(self.params, gparam_momentum):\n                updates[param] = param - lr*gparam_mom\n            self.train = theano.function(inputs = [X, Y, epoch], outputs = cost, updates=updates)\n            \n        self.predictions = theano.function(inputs = [X], outputs = y_vals)    \n        self.get_lr = theano.function(inputs = [], outputs = lr)\n        \n            \n    def sample_words(self, grammar, n_words, max_len=1000):\n        init = grammar.get_char_one_hot('B')[0]\n        words = []\n        for j in range(n_words):\n            word = [init]\n            count = 0\n            for i in range(max_len):\n                probas = self.predictions(word)[-1]\n                probas = probas / np.sum(probas)\n                letter = np.random.multinomial(n=1, pvals=probas)\n                word.append(letter)\n                if np.equal(letter, grammar.get_char_one_hot('.')[0]).all():\n                    break\n                \n            words.append(grammar.sequenceToWord(word))\n        return words\n\n    def save(self, path):\n        f = file(path, 'wb')\n        objects = [p.get_value() for p in self.params + [self.h0]]\n        objects.append(self.optim)\n        for obj in objects:\n            cPickle.dump(obj, f, protocol=cPickle.HIGHEST_PROTOCOL)\n        f.close()", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": 93, "cell_type": "code", "source": "train_data = grammar.get_n_examples(10000)\ntest_data = grammar.get_n_examples(500)\n# We remove from our test data the observations that occured in the training data\nltrain = set(\"\".join(str(tt.argmax()) for tt in t[0]) for t in train_data)\nltest = list(\"\".join(str(tt.argmax()) for tt in t[0]) for t in test_data)\ntest_data = [t for (t, lt) in zip(test_data, ltest) if not lt in ltrain]", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": 196, "cell_type": "code", "source": "# Later we shall optimize hyperparameters with Spearmint\noptim = {\"method\":\"momentum\", \"learning_rate\":0.05, \"mom_start\":0.5, \"mom_end\": 0.9, \"mom_epoch_interval\":50}\nmodel = Rnn(optim=optim, n_in=len(grammar.chars), n_hid=60, n_out=len(grammar.chars))", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "epoch = 0\ntraining_errors = []\ntest_likelihood = []\nsampling_precision = []\nearly_stopping = False\npatience = 4*len(train_data)\nmax_test_likelihood = -np.Inf\nwhile epoch<1000 and not early_stopping:\n    epoch += 1\n    errors = []\n    for i in range(len(train_data)):\n        idx = np.random.randint(0, len(train_data))\n        x, y = train_data[idx]\n        errors.append(model.train(x, y, epoch))\n    iter = epoch*len(train_data)\n    training_errors.append(np.sum(errors))\n    likelihood = []\n    for t in test_data:\n        table = np.array(model.predictions(t[0])*t[1])\n        likelihood.append(np.mean(np.sum(table, axis=1)))\n    test_likelihood.append(np.mean(likelihood))\n    if test_likelihood[-1]>max_test_likelihood:\n        if 0.9999*test_likelihood[-1]>max_test_likelihood:\n            patience = max(patience, 4*iter)\n        max_test_likelihood = test_likelihood[-1]\n#         model.save('RNN_reber.pickle')\n    samples = model.sample_words(grammar, 100)\n    is_in_grammar = [grammar.in_grammar(w) for w in samples]\n    sampling_precision.append(np.mean(is_in_grammar))\n    if not epoch%10:\n        print \"Epoch \",epoch, \", \\n\\tTraining error :\", training_errors[-1], \"\\n\\tTest likelihood :\", test_likelihood[-1]\n        print \"\\tSampling precision :\",sampling_precision[-1]\n        print \"\\tPatience :\", patience, \", Iter :\", iter\n        plt.subplot(131)\n        train_plot, = plt.plot(range(epoch), training_errors, label=\"Training error\", color='g')\n        train_plot.figure.set_size_inches(16.,4.)\n        plt.legend()\n        plt.subplot(132)\n        test_plot, = plt.plot(range(epoch), test_likelihood, label=\"Test likelihood\", color='r')\n        plt.legend()\n        plt.subplot(133)\n        sampling_plot, = plt.plot(range(epoch), sampling_precision, label=\"Sampling precision\", color='b')\n        plt.legend()\n        plt.show()\n    if patience<=iter:\n        early_stopping = True", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.6", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}